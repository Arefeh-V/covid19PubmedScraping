{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TxqkA48h-TLY"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "import urllib.request, urllib.parse, urllib.error\n",
        "from urllib3.util.retry import Retry\n",
        "import re\n",
        "import ssl\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "from collections import Counter\n",
        "import string\n",
        "import codecs\n",
        "from itertools import chain\n",
        "from google.colab import files\n",
        "# import Levenshtein as lev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7dEEIm1m-rFv"
      },
      "outputs": [],
      "source": [
        "#  take a soup and extracts all needed elements for the bibliography and abstract\n",
        "def get_bibliography(soup):\n",
        "    \n",
        "    article = soup.find('article')\n",
        "\n",
        "##################### country #########################\n",
        "    country = ''\n",
        "    if soup.find('country'):\n",
        "      country = soup.find('country').text\n",
        "    \n",
        "###################### authors ########################\n",
        "    authors = []\n",
        "    authorlist = soup.find('authorlist')\n",
        "    if authorlist:\n",
        "      for i in range(len(authorlist.find_all('author'))):\n",
        "          currentAuthor = authorlist.find_all('author')[i]\n",
        "          \n",
        "          ForeName = ''\n",
        "          if currentAuthor.find('forename'):\n",
        "            ForeName = currentAuthor.find('forename').text \n",
        "          \n",
        "          lastname = ''\n",
        "          if currentAuthor.find('lastname'):\n",
        "            lastname = currentAuthor.find('lastname').text\n",
        "\n",
        "          initial = ''\n",
        "          if currentAuthor.find('initials'):\n",
        "              initial = currentAuthor.find('initials').text\n",
        "\n",
        "          Affiliation = []\n",
        "          if currentAuthor.find('affiliation'):\n",
        "            for i in range(len(currentAuthor.find_all('affiliation'))):\n",
        "              # keyword = keywordlist.find_all('keyword')[i].text\n",
        "              Affiliation.append(currentAuthor.find_all('affiliation')[i].text)\n",
        "\n",
        "          author={'lastname':lastname,'ForeName':ForeName,'initial':initial,'Affiliation':Affiliation }\n",
        "          authors.append(author)\n",
        "\n",
        "\n",
        "##################### keywords #########################\n",
        "    keywords = []\n",
        "    keywordlist = soup.find('keywordlist')\n",
        "    if keywordlist:\n",
        "        for i in range(len(keywordlist.find_all('keyword'))):\n",
        "            keyword = keywordlist.find_all('keyword')[i].text\n",
        "            #  keyword = keyword.replace('\\u2013','-').replace('\\u00e2\\u20ac\\u201c','-')\n",
        "            keywords.append(str(keyword))\n",
        "\n",
        "\n",
        "###################### references ########################\n",
        "    references = []\n",
        "    referencelist = soup.find('referencelist')\n",
        "    if referencelist:\n",
        "        for i in range(len(referencelist.find_all('articleidlist'))):\n",
        "          reference = {\n",
        "              \"pubmed\": \"\",\n",
        "              \"doi\": \"\"\n",
        "              }\n",
        "          # print(referencelist.find_all('articleidlist')[i].parent.name)\n",
        "          if(referencelist.find_all('articleidlist')[i].find('articleid', {'idtype':'pubmed'})):\n",
        "            reference[\"pubmed\"] = referencelist.find_all('articleidlist')[i].find('articleid', {'idtype':'pubmed'}).text \n",
        "          if(referencelist.find_all('articleidlist')[i].find('articleid', {'idtype':'doi'})):\n",
        "            reference[\"doi\"] = referencelist.find_all('articleidlist')[i].find('articleid', {'idtype':'doi'}).text\n",
        "          references.append(reference)\n",
        "\n",
        "\n",
        "#################### ArticleTitle ##########################\n",
        "    ArticleTitle = ''\n",
        "    if article:\n",
        "        ArticleTitle = article.find('articletitle').text\n",
        "        # print(ArticleTitle)\n",
        "\n",
        "\n",
        "###################### journal_title ########################\n",
        "    journal_title = ''\n",
        "    if soup.find('title'):\n",
        "        journal_title = soup.find('title').text\n",
        "    \n",
        "\n",
        "###################### year ########################\n",
        "    year=''\n",
        "    if(soup.find('pubdate')):\n",
        "      if(soup.find('pubdate').find('year')):\n",
        "        year = soup.find('pubdate').find('year').text\n",
        "    elif soup.find('pubmedpubdate'):\n",
        "      if(soup.find('pubmedpubdate').find('year')):\n",
        "        year = soup.find('pubmedpubdate').find('year').text\n",
        "\n",
        "\n",
        "##################### pubmed & doi #########################\n",
        "    pubmed = ''\n",
        "    doi = ''\n",
        "    if soup.find('articleidlist'):\n",
        "      \n",
        "      if (soup.find('articleidlist').find('articleid', {'idtype':'pubmed'}) and soup.find('articleidlist').parent.name == 'pubmeddata'):\n",
        "        pubmed = soup.find('articleidlist').find('articleid', {'idtype':'pubmed'}).text\n",
        "          \n",
        "      if (soup.find('articleidlist').find('articleid', {'idtype':'doi'}) and soup.find('articleidlist').parent.name == 'pubmeddata'):\n",
        "        doi = soup.find('articleidlist').find('articleid', {'idtype':'doi'}).text\n",
        "\n",
        "\n",
        "##############################################            \n",
        "    result = {\n",
        "        'ArticleTitle':ArticleTitle,\n",
        "        'journal_title':journal_title,\n",
        "        'pubmed':pubmed,\n",
        "        'doi':doi,\n",
        "        'country':country,\n",
        "        'authors':authors,\n",
        "        'references':references,\n",
        "        'keywords':keywords,\n",
        "        'year':year\n",
        "        \n",
        "    }\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fetchData(id):\n",
        "\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "    url = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id=itemid\"\n",
        "    url = url.replace('itemid', id)\n",
        "\n",
        "    try:\n",
        "        _create_unverified_https_context = ssl._create_unverified_context\n",
        "    except AttributeError:\n",
        "        # Legacy Python that doesn’t verify HTTPS certificates by default\n",
        "        pass\n",
        "    else:\n",
        "        # Handle target environment that doesn’t support HTTPS verification\n",
        "        ssl._create_default_https_context = _create_unverified_https_context\n",
        "      \n",
        "    r = ''\n",
        "    while r == '':\n",
        "      try:\n",
        "        session = requests.Session()\n",
        "        retry = Retry(connect=3, backoff_factor=0.5)\n",
        "        adapter = HTTPAdapter(max_retries=retry)\n",
        "        session.mount('http://', adapter)\n",
        "        session.mount('https://', adapter)\n",
        "        response = session.get(url)\n",
        "        break\n",
        "      except Exception as e:\n",
        "        print('fetching error:: ', str(e))\n",
        "        print(\"Connection refused by the server..\")\n",
        "        print(\"sleep for 2 seconds\")\n",
        "        time.sleep(2)\n",
        "        print(\"continue...\")\n",
        "\n",
        "    page_xml = response.content\n",
        "    soup = BeautifulSoup(page_xml, \"html.parser\")\n",
        "   \n",
        "    articleData = get_bibliography(soup)\n",
        "     \n",
        "    return articleData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqG-y2th-vnd"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  url=\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json&retmax=NUM&sort=relevance&term=KEYWORD\"\n",
        "\n",
        "  # ask the user to provide the keyword and number of results and subsequently replace these elements in the url string\n",
        "  keyword = str(input('Please enter the keyword '))\n",
        "  num = int(input('Please enter the number of results '))\n",
        "  url = url.replace('NUM', str(num))\n",
        "  url = url.replace('KEYWORD', keyword)\n",
        "\n",
        "\n",
        "  try:\n",
        "      _create_unverified_https_context = ssl._create_unverified_context\n",
        "  except AttributeError:\n",
        "      # Legacy Python that doesn’t verify HTTPS certificates by default\n",
        "      pass\n",
        "  else:\n",
        "      # Handle target environment that doesn’t support HTTPS verification\n",
        "      ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "  webpage = urllib.request.urlopen(url).read()\n",
        "  dict_page =json.loads(webpage)\n",
        "  idlist = dict_page[\"esearchresult\"][\"idlist\"]\n",
        "\n",
        "  fold = math.ceil(num/2)\n",
        "  for i in range(2):\n",
        "    selected_idlist = idlist[fold*i:fold*(i+1)] \n",
        "    articles_list = []\n",
        "    \n",
        "    for id in selected_idlist:\n",
        "      # time.sleep(1)\n",
        "      counter = 0\n",
        "      article = {\"pubmed\":\"\"}\n",
        "      while article[\"pubmed\"] == \"\":\n",
        "        if counter < 5:\n",
        "          article = fetchData(id)\n",
        "          counter == counter+1\n",
        "        else:\n",
        "          break\n",
        "      print(selected_idlist.index(id)+fold*i,\" : \" , id)\n",
        "      article['id'] =str(selected_idlist.index(id)+fold*i)\n",
        "      articles_list.append(article)\n",
        "\n",
        "    # Serializing json\n",
        "    json_object = json.dumps(articles_list, indent=4)\n",
        "\n",
        "    # Writing to sample.json\n",
        "    file_name = str(fold*i)+\"to\"+ str(fold*(i+1)) + '.json'\n",
        "    with open(file_name, \"w\") as outfile:\n",
        "        outfile.write(json_object)\n",
        "      \n",
        "    files.download(file_name) \n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pre processing the results\n"
      ],
      "metadata": {
        "id": "WtEOGiufx5yB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MuDnpI5eDKqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "with open('/content/drive/My Drive/Colab Notebooks/data_4262/covid19_ml_4262.json') as f:\n",
        "    text = f.read()\n",
        "    data= json.loads(text)"
      ],
      "metadata": {
        "id": "QFBpHIpwR2Qh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the article has our keywords either in its title or keywordlist \n",
        "kw = 'covid machine learning'\n",
        "filterByKwData=[]\n",
        "\n",
        "for item in data:\n",
        "   \n",
        "    kwsplit= kw.split(' ')\n",
        "    itemKwList = ' '.join(item['keywords']).lower()\n",
        "    titlecounter = 0\n",
        "    kwcounter = 0\n",
        "    for kwitem in kwsplit:\n",
        "        if kwitem in item['ArticleTitle'].lower():\n",
        "            titlecounter = titlecounter + 1\n",
        "        if kwitem in itemKwList:\n",
        "            kwcounter = kwcounter + 1\n",
        "    if titlecounter >=2 or kwcounter >=2:\n",
        "        filterByKwData.append(item) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('included: ', len(filterByKwData))\n",
        "json_object = json.dumps(filterByKwData, indent=4)\n",
        "included_file_name = 'included_'+str(len(filterByKwData))+'.json'\n",
        "with open(included_file_name, \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "files.download(included_file_name) \n",
        "\n",
        "\n",
        "# get sublist which doesnt have the keyword\n",
        "not_included = [x for x in data if x not in filterByKwData]\n",
        "print('not included: ', len(not_included))\n",
        "json_object = json.dumps(not_included, indent=4)\n",
        "not_included_file_name = 'not_included_'+str(len(not_included))+'.json'\n",
        "with open(not_included_file_name, \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "files.download(not_included_file_name) \n",
        "\n",
        "\n",
        "# extract list of articlea with empty keyword for further review\n",
        "included_null_keywords = [x for x in filterByKwData if x['keywords']== []]\n",
        "print('include null keyword: ', len(included_null_keywords))\n",
        "json_object = json.dumps(included_null_keywords, indent=4)\n",
        "included_null_keywords_file_name = 'not_included_'+str(len(included_null_keywords))+'.json'\n",
        "with open(included_null_keywords_file_name, \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "files.download(included_null_keywords_file_name) "
      ],
      "metadata": {
        "id": "pq1Wt7uFyYgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check number af empty elements in our data\n",
        "allitems=0\n",
        "nullArticleTitle=[]\n",
        "nulljournal_title=[]\n",
        "nullpubmed=[]\n",
        "nullcountry=[]\n",
        "nulldoi_pii_str=[]\n",
        "nullauthors=[]\n",
        "nullreferences=[]\n",
        "nullkeywords=[]\n",
        "nullrfkw=[]\n",
        "nullyear=[]\n",
        "for item in filterByKwData:\n",
        "    allitems = allitems + 1\n",
        "\n",
        "    if item['ArticleTitle']=='':\n",
        "        nullArticleTitle.append(item)\n",
        "\n",
        "    if item['journal_title']=='':\n",
        "        nulljournal_title.append(item)\n",
        "\n",
        "    if item['pubmed']=='':\n",
        "        nullpubmed.append(item)\n",
        "\n",
        "    if item['country']=='':\n",
        "        nullcountry.append(item)\n",
        "\n",
        "    if len(item['authors']) ==0:\n",
        "        nullauthors.append(item)\n",
        "\n",
        "    if len(item['references']) ==0:\n",
        "        nullreferences.append(item)\n",
        "\n",
        "    if len(item['keywords']) ==0:\n",
        "        nullkeywords.append(item)\n",
        "\n",
        "    if len(item['keywords']) ==0 and len(item['references']) ==0:\n",
        "        nullrfkw.append(item)\n",
        "\n",
        "    if item['year'] =='':\n",
        "        nullyear.append(item)\n",
        "\n",
        "        \n",
        "print('all : ', allitems)\n",
        "print('nullArticleTitle : ', len(nullArticleTitle))\n",
        "print('nulljournal_title : ', len(nulljournal_title))\n",
        "print('nullpubmed : ', len(nullpubmed))\n",
        "print('nullcountry : ', len(nullcountry))\n",
        "print('nulldoi_pii_str : ', len(nulldoi_pii_str))\n",
        "print('nullauthors : ', len(nullauthors))\n",
        "print('nullreferences : ', len(nullreferences))\n",
        "print('nullkeywords : ', len(nullkeywords))\n",
        "print('nullyear : ', len(nullyear))"
      ],
      "metadata": {
        "id": "K2JBL_Puye1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract list of unique countries with their frequently distribution\n",
        "all_countries=[item['country'] for item in  filterByKwData]\n",
        "countries_distribution = Counter(all_countries)\n",
        "all_unique_countries = np.unique(all_countries)\n",
        "countries_distribution_num = len(countries_distribution.keys())  \n",
        "countries_distribution_df = pd.DataFrame(columns=['Key', 'Value'])\n",
        "for i,j,k in zip(range(countries_distribution_num), countries_distribution.keys(), countries_distribution.values()):\n",
        "    countries_distribution_df.loc[i] = [j, k]\n",
        "countries_distribution_df.to_csv('countries_distribution.csv') \n",
        "files.download('countries_distribution.csv')\n",
        "\n",
        "\n",
        "# extract list of unique journals with their frequently distribution\n",
        "all_journals = [item['journal_title'] for item in  filterByKwData]\n",
        "journals_distribution = Counter(all_journals)\n",
        "all_unique_journals = np.unique(all_journals)\n",
        "journals_distribution_num = len(journals_distribution.keys())  \n",
        "journals_distribution_df = pd.DataFrame(columns=['Key', 'Value'])\n",
        "for i,j,k in zip(range(journals_distribution_num), journals_distribution.keys(), journals_distribution.values()):\n",
        "    journals_distribution_df.loc[i] = [j, k]\n",
        "journals_distribution_df.to_csv('journals_distribution.csv') \n",
        "files.download('journals_distribution.csv')\n",
        "\n",
        "\n",
        "# extract list of unique years with their frequently distribution\n",
        "all_years = [item['year'] for item in  filterByKwData]\n",
        "years_distribution = {i:all_years.count(i) for i in all_years}\n",
        "years_distribution = Counter(all_years)\n",
        "years_distribution_num = len(years_distribution.keys())  \n",
        "years_distribution_df = pd.DataFrame(columns=['Key', 'Value'])\n",
        "for i,j,k in zip(range(years_distribution_num), years_distribution.keys(), years_distribution.values()):\n",
        "    years_distribution_df.loc[i] = [j, k]\n",
        "years_distribution_df.to_csv('years_distribution.csv') \n",
        "files.download('years_distribution.csv')\n",
        "\n",
        "\n",
        "# extract list of unique keyword with their frequently distribution\n",
        "keywordslist = []\n",
        "keywordslist_with_articleid = []\n",
        "for item in filterByKwData:\n",
        "    for kw_item in item['keywords']:\n",
        "        kw = kw_item.lower()\n",
        "        keywordslist.append(kw)\n",
        "        keywordslist_with_articleid.append((item['pubmed'] , kw))\n",
        "\n",
        "unique_keywordslist = [pair[0] for pair in sorted(Counter(keywordslist).items(), key=lambda item: item[1], reverse=True)]\n",
        "keyword_distribution = Counter(keywordslist)\n",
        "\n",
        "keyword_distribution_num = len(keyword_distribution.keys())  \n",
        "keyword_distribution_df = pd.DataFrame(columns=['Key', 'Value'])\n",
        "for i,j,k in zip(range(keyword_distribution_num), keyword_distribution.keys(), keyword_distribution.values()):\n",
        "    keyword_distribution_df.loc[i] = [j, k]\n",
        "keyword_distribution_df.to_csv('keyword_distribution.csv') \n",
        "files.download('keyword_distribution.csv')\n",
        "\n",
        "\n",
        "# extract all keywords in data with its article pubmed id for further preprocessing like detecting semanticaly similar keywords \n",
        "keywordslist_with_articleid_df = pd.DataFrame(keywordslist_with_articleid, columns=('articlePubmedId', 'keyword'))\n",
        "keywordslist_with_articleid_df.to_csv('keywordslist_with_articleid.csv') \n",
        "files.download('keywordslist_with_articleid.csv')\n",
        "\n",
        "\n",
        "\n",
        "print(countries_distribution)\n",
        "print(journals_distribution)\n",
        "print(years_distribution)\n",
        "print(Counter(keywordslist))\n",
        "print(len(unique_keywordslist))"
      ],
      "metadata": {
        "id": "iPRdeShkyojJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract last item of authors affiliation splitted by comma (which is mostly their country) in order to detect co-countries network\n",
        "countrylist = []\n",
        "countrylist_with_articleid = []\n",
        "for item in filterByKwData:\n",
        "  for author in item['authors']:\n",
        "    for x in author['Affiliation']:\n",
        "      affiliation_item = {\n",
        "          \"pubmed\":item[\"pubmed\"], \n",
        "          \"firstAffil\": x.split(',')[len(x.split(','))-1].replace('.', '').strip().lower(), \n",
        "          \"secondAffil\": x.split(',')[len(x.split(','))-2].strip().lower()\n",
        "          } \n",
        "      countrylist_with_articleid.append(affiliation_item)\n",
        "\n",
        "countrylist_with_articleid_df = pd.DataFrame(countrylist_with_articleid)\n",
        "countrylist_with_articleid_df.to_csv('countrylist_with_articleid.csv') \n",
        "files.download('countrylist_with_articleid.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xpN_ceTtrZZ3",
        "outputId": "d152fc93-9f6b-46bf-93c3-e2c2f0b9ca48"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2f7a1587-b027-4d2e-b2b8-ab9456b59b7a\", \"countrylist_with_articleid.csv\", 891895)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#keywordlist_with_articleid co-keyword analysis source target extraction\n",
        "keywordslist_with_articleid = []\n",
        "with open('/content/drive/My Drive/Colab Notebooks/data_4262/keywordlist_with_articleid.csv') as file:\n",
        "    keywordslist_with_articleid = pd.read_csv(file)\n",
        "keywordslist_with_articleid.head()"
      ],
      "metadata": {
        "id": "ov-8-sQxlNzm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kwdf_pubmed = list(keywordslist_with_articleid['pubmed'].unique())\n",
        "kwdf_keyword = list(keywordslist_with_articleid['keyword'].unique())\n",
        "kw_lst = list(keywordslist_with_articleid)\n",
        "co_keywords = []\n",
        "for id in kwdf_pubmed:\n",
        "  id_kws = list(keywordslist_with_articleid.loc[keywordslist_with_articleid['pubmed'] == id]['keyword'])\n",
        "  for index1, kw1 in enumerate(id_kws):\n",
        "    for kw2 in id_kws[index1+1:]:\n",
        "      co_keywords.append((kw1.strip(),kw2.strip()))\n",
        "\n",
        "print(len(co_keywords))"
      ],
      "metadata": {
        "id": "wCbX2EnXmZWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(co_keywords).most_common())\n",
        "kw_df1 = pd.DataFrame(Counter(co_keywords).most_common(), columns=[\"tuple\", \"weight\"])\n",
        "final_kw_df = pd.DataFrame(list(kw_df1['tuple']), columns=[\"source\", \"target\"])\n",
        "final_kw_df[\"weight\"] = kw_df1[\"weight\"]\n",
        "final_kw_df.head()\n",
        "final_kw_df.to_csv('co_keyword.csv') \n",
        "files.download('co_keyword.csv')"
      ],
      "metadata": {
        "id": "FbCYI-hLrA2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try to co-keyword from large json data  itself which took so long \n",
        "lst = keywordslist_with_articleid\n",
        "co_keywors_list = []\n",
        "\n",
        "for index1, kw1 in enumerate(lst):\n",
        "  for kw2 in lst[index1+1:]:\n",
        "    print(index1)\n",
        "    weight = 0\n",
        "    for item in filterByKwData:\n",
        "      itemKwList = item[\"keywords\"]\n",
        "      if kw1 in itemKwList and kw2 in itemKwList:\n",
        "        weight +=1\n",
        "    if weight > 0:\n",
        "      co_keywors_list.append((kw1,kw2, weight))\n",
        "\n",
        "# kw_df = pd.DataFrame(co_keywors_list, columns=('kw1', 'kw2', 'weight'))\n",
        "# kw_df.to_csv('co_keywords.csv') \n",
        "# files.download('co_keywords.csv')\n"
      ],
      "metadata": {
        "id": "ek4VyO5bsxy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try levenshtain algorithm \n",
        "def clean_text(txt):\n",
        "    txt = txt.strip()\n",
        "    txt = txt.translate(str.maketrans('', '', string.punctuation))\n",
        "    return txt.lower()\n",
        "\n",
        "\n",
        "for kw1 in unique_keywordslist[:100]:\n",
        "    # print(\"kw1: \", kw1)\n",
        "    for kw2 in unique_keywordslist[:100]:\n",
        "        kw1 = clean_text(kw1)\n",
        "        kw2 = clean_text(kw2)\n",
        "        if lev.distance(kw1, kw2) > 0 and lev.distance(kw1, kw2) <=5:\n",
        "            print(kw1,\" vs \", kw2 ,\" : \", lev.distance(kw1, kw2))\n",
        "            print(\"delete: \", kw2)\n",
        "            try:\n",
        "                unique_keywordslist.remove(kw2)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "print(len(unique_keywordslist))\n"
      ],
      "metadata": {
        "id": "lznQ27rIy1m7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}